[[08. 맵리듀스 튜닝하기]]

이번 장에서는 하둡을 운영할 때 필요한 실무적인 내용들을 살펴보겠습니다.

## 9.1 네임노드 HA 구성
네임노드는 하둡에서 가장 중요한 구성 요소입니다.

따라서, 네임노드에 장애가 생기면 HDFS에 저장된 데이터 모두 무용지물이 됩니다. 이런 이유 때문에 네이노드를 하둡의 Single Point of Failure (SPOF)이라 하며, 그야말로 해당 부분이 실패하면 시스템 전체가중단되는 요소입니다.

이런 단점을 극복하기 위해 하둡은 네임노드가 실패하지 않거나 실패해도 대안이 있도록 구성해야 하는데, 이런 성질을 HA (High Availability)라 부른다.

즉, 장애가 발생하더라도 서비스를 운영할 수 있는 시스템을 의미합니다. 

이런 HA를 구성하는 방식으로는 백업, 장애 대처 방법, 데이터 저장 및 접근에 대한 방식 등이 있으며 파일 시스템에 대해서는 RAID, 또는 3장에서 배운 SAN 등이 있습니다.

하둡에서는 다음과 같은 HA 구성 방법이 존재합니다:
 - 아파치: 하둡 2.0에서 네임노드의 HA 제공.
 - 페이스북: 아바타 노드
 - 야후!: 기존 리눅스의 HA를 이용한 네임노드 HA 프로토타입 구현
 - 이베이: 백업노드와 룩 리포트 복제기 (replicator)를 이용한 네임노드 HA 구현
이번에는 특히 많이 사용되는 페이스북의 아바타 노드에 대해 다뤄볼 것입니다.


### 9.1.1 아바타 노드의 등장 배경
페이스북에서 하둡 클러스터를 재구동하는데 거의 한시간 가까이 걸렸는데 하둡의 백업노드는 성에 안차서 자체적으로 해결책을 개발하게 되었으며 아바타 노드가 탄생하게 되었습니다.

### 9.1.2 아바타 노드의 작동 방식
아바타 노드는 네임노드의 래퍼 클래스이기 때문에 기존의 네임노드 코드를 수정하지 않고 래핑만 하여 네임노드의 기능이 사용 가능합니다. 백업노드는 네이스페이스와 블록 정보를 관리하지 못했지만 아바타 노드는 네임노드를 래핑하는 형태로 이를 극복했습니다. 


- 아바타 노드
기본 아바타 노드 (PrimaryAvatarNode)와 대기 아바타 노드 (StandbyAvatarNode)로 나뉩니다.
Active 상태인 노드에 장애가 발생하면 Standby 노드가 Active 상태가 된다. 이런 장애가 발생하는지 감시하는건 주키퍼가 합니다.
이 둘의 파일 네임스페이스를 동일하게 유지하기 위해 기본 아바타노드가 대기 아바타노드에게 NFS (Network File System)에 HDFS의 트랜잭션 로그를 저장합니다.

- 클라이언트
클라이언트는 HDFS 트랜잭션을 새로 시작하거나 오류가 나서 중간에 시작할 때도 주키퍼를 체크합니다.

- 아바타 데이터노드
NFS를 이요해 트랜잭션을 기록만 해서는 시스템 네임스페이스를 완벽하게 유지할 수 없기 때문에 아바타 데이터노드가 등장합니다. 기존의 데이터노드는 하나의 네임노드와 통신을 한 반면, 아바타 데이터노드는 기본 아바타노드와 보조 아바타노드와 동시에 통신을 합니다. 따라서, 두개의 아바타 노드가 HDFS의 블록 정보를 유지할 수 있게 도와줍니다. 

- 장애 복구
장애 복구는 아바타 셸 프로그램을 이용해 수동으로 복구합니다.

## 9.2 파일 시스템 상태 확인
HDFS에 저장된 파일에는 다양한 문제가 발생할 수 있으며, 해당 파일들을 확인할 때 쓰이는 명령어가 fsck입니다. 다음과 같이 명령어를 실행하면 됩니다.

`./bin/hadoop fsck /`

파라미터가 `/` 인 이유는 반드시 경로가 필요해서입니다.

이때 돌아올 수 있는 출력 결과는: 1) Over-replicated blocks, 2) Under-replicated blocks, 3) Mis-replicated blocks, 4) Corrupt blocks 등이 있습니다. 
1과 2의 경우에는 하둡이 자동으로 정리를 해서 크게 문제가 되지 않지만 3번과 4번의 경우에는 파일을 조회하지 못하는 상황이 될 수도 있습니다. 따라서 데이터를 조회하지 못하고 오류난 블록을 일괄적으로 삭제하는 기능을 fsck에서 제공합니다. -delete 옵션을 지정하면 삭제됩니다.

1&2에 대한 경우에는 블록을 정리할 수 있는 밸런서 명령어가 존재합니다.
`./bin/hadoop balancer -threshold [threshold]`

threshold는 블록을 과도하게 사용하고 있는 데이터노드와 정상적인 데이터노드 간의 사용 비율 차이를 얼마나 줄 것인지를 나타내며, 기본값은 10%입니다.

## 9.3 HDFS admin 명령어 사용

HDFS의 관리자 권한 명령어로 dfsadmin 을 제공합니다. 

가장 많이 사용되는 명령어들은 다음과 같습니다:
### 9.3.1 report
HDFS의 기본적인 정보와 상태를 출력합니다.
`./bin/hadoop dfsadmin -report`

### 9.3.2 safemode
하둡을 재구동하면 로컬에 저장된 파일 시스템 이미지와 에디트 로그를 조회한 후 메모리에 있는 파일 시스템 이미지를 갱신합니다. 이때 데이터노드는 저장하고 있는 블록의 위치 정보를 네임노드에 전송합니다.

네임노드는 [메모리에 올라와 있는 파일 시스템 이미지] 와 [데이터노드가 전송한 블록의 정보]를 비교하는 작업을 수행하며, 이런 작업을 ***블록 리포팅***이라고 합니다. 

이런 블록 리포팅이 되기 전의 상태를 안전모드 (safemode)라고 합니다.

안전모드를 다음과 같이 시작/중지 할 수 있습니다. 중지할 경우에는 enter 대신 leave.

`./bin/hadoop dfsadmin -safemode enter

### 9.3.3 saveNamespace
[로컬 파일 시스템에 저장돼 있는 파일 시스템 이미지 파일]과 [에디트 로그]를 ***현재 버전***으로 갱신할 수 있는 saveNamespace 명령어를 제공합니다. 단, saveNamespace 명령어는 실행하려면 반드시 safemode에서 진행해야 합니다. 

`./bin/hadoop dfsadmin -saveNamespace`

### 9.3.4 파일 저장 개수 설정
HDFS의 디렉터리에 파일이 과도하게 생성되는 것을 제한할 수 있는 쿼터 설정 명령어를 제공합니다. 주의할 점은 파라미터로 사용하는 쿼터수가 지정된 디렉터리까지 포함하기 때문에 1로 설정하면 아무 파일도 생성되지 않습니다. 디렉토리가 1로 들어가기 때문에.

`./bin/hadoop -setQuota [쿼터수] [디렉터리명]`

### 9.3.5 파일 저장 용량 설정
디렉토리에 저장할 파일 크기까지 설정할 수 있습니다.
이때, 용량은 숫자 뒤에 MB이면 m, GB이면 g, TB이면 t를 붙이면 된다. 즉 1.5GB는 1500m이라고 쓸 수 있다. 

`./bin/hadoop dfsadmin -setSpaceQuota [용량] [디렉터리명]`

## 9.4 데이터 저장 공간 관리
하둡 클러스터를 설정할때 경로 관련 속성을 주의해서 설정해야합니다. 

예를 들어, hadoop.tmp.dir의 기본값은 리눅스 서버의 임시 데이터 저장 공간인 /tmp 디렉토리인데 /tmp 경로에는 기본적으로 용량이 적게 할당돼 있기 때문에 오류가 뜰 수 있습니다. 따라서, /tmp를 사용하지 않고 하둡 관리자의 홈에 있는 디렉토리로 설정하는 것이 좋습니다.


## 9.5 데이터노드 제거
하둡을 운영하다보면 특정 데이터노드를 HDFS에서 제거해야 할 때가 있는데, 이럴때마다 하둡 자체를 중단시키고 재시작하는 것은 비효율적입니다. 데이터노드를 제거하려면 dfs.hosts.exclude 설정을 hdfs-site.xml에 추가하여 재시작하는 것이 더 좋은 방법입니다.

dfs.hosts.exclude 관련 설정 예시
`<property>
	`<name>dfs.hosts.exclude</name>`
	`<value>/home/hadoop/hadoop/conf/exclude_server</value>`
`</property>`

재시작하는 명령어는 다음과 같습니다:
`./bin/hadoop dfsadmin -refreshNodes`

## 9.6 데이터노드 추가
운영 중인 하둡에 다음과 같은 방법으로 간단히 데이터노드를 추가할 수 있습니다. 
1. 네임노드의 slaves 파일에 데이터노드를 추가한다. 
2. 데이터노드의 환경 설정 파일에 네임노드 접속 주소를 설정한다.
3. 데이터노드용 서버에서 데이터노드와 태스크트래커를 구동한다.

### 9.7 네임노드 장애 복구
가장 이상적인 방법은 HA를 구성하는거지만 그러지 못할 경우도 존재합니다.

### 9.7.1 네임노드와 보조 네임노드 데이터 구조
네임노드의 장애를 복구하기 위해서는 네임노드의 메타데이터가 어떤 구조로 저장돼 있는지 이해하고 있어야 합니다. 
dfs.name.dir의 기본값은 ${hadoop.tir.dir}/dfs/name이며, current 디렉토리에는 HDFS에서 마지막으로 크포인팅한 데이터가 저장됩니다.


### 9.7.2 보조 네임노드를 이용한 장애 복구
보조 네임노드를 이용하여 네임노드 장애 복구를 하려면 다음과 같이 하면 됩니다. 

1. 하둡 전체 데몬 실행하지 않고 네임노드만 실행한다.
2. 수동으로 dfs/name 디렉토리를 생성한다.
3. -importCheckpoint 옵션을 추가해서 네임노드를 실행합니다. 이 옵션은 보조 네임노드에 저장된 체크포인트를 이요해 네임노드를 실행하겠다는 의미다.
4. 네임노드가 정상적으로 구동됐으면 이를 중지하고 하둡 전체 데몬을 재실행합니다.
5. 기존에 삭제된 dfs.name.dir 디렉터리를 조회하면 원래의 구조대로 디렉터리가 생성되어 있습니다. 

### 9.7.3 NFS를 이용한 장애 복구
위와같이 장애 복구를 해도 데이터 유실의 가능성이 존재합니다. 체크포인트가 한시간 단위로 진행이 된다 하면 그 사이에 일어나는 변화는 유실될 수 있기 때문입니다. 이런 경우를 대비하여 NFS를 이용하면 됩니다. 

NFS(Network File System)
NFS를 이용해 네임노드의 메타데이터를 이중화할 수 있습니다. 9.10와 같이 dfs.name.dir을 수정합니다. 

1. 기존 네임노드 서버의 네트워크를 차단합니다. 
2. 백업 서버의 IP를 기존 네임노드 서버의 IP로 할당.
3. 백업 서버에 네임노드를 설치합니다. 
4. dfs.name.dir를 NFS로 마운트돼 있는 디렉터리로 설정합니다.
5. 네임노드 데몬을 실행.
6. 네임노드가 정상적으로 구동돼 있는지 확인합니다.


## 9.8 데이터노드 장애 복구
데이터노드의 장애는 네임노드의 장애보다 복구하기 쉽습니다.
1. 해당 데이터노드를 중지시킵니다. 
2. 손상된 디스크를 언마운트하고 새로운 디스크를 마운트합니다. 
3. 새로운 디스크에 dfs.data.dir 속성으로 설정한 디렉터리를 생성합니다. 
4. 데이터노드를 재실행합니다. 
5. 네임노드 서버에서 밸런서를 실행합니다.
6. HDFS 웹 UI에서 해당 데이터노드가 조회되는지 확인합니다.

## 9.9 하둡 사용자 관리
하둡 사용자는 하둡 명령어를 실행하거나 하둡 API를 실해하는 리눅스 계정이기 때문에 여러 사용자가 있을 수 있습니다. 그 중 하둡 데몬을 실행하는게 superuser로 분류됩니다. HDFS 내에 사용자의 홈 디렉터리는 /user/${username} 으로 생성됩니다. 

## 9.10 하둡 주요 포트
하둡은 내부 데몬 간에 통신을 위해 다양한 포트를 사용합니다. 하둡이 사용하는 포트가 막혀 있다면 하둡을 구동하더라도 HDFS 파일 제어나 맵리듀스 잡이 정상적으로 실행되지 않을 확률이 매우 높습니다. 

dfs.secondary.http.address (50090), dfs.datanode.address (50010) 등 다양한 포트가 존재하는데, 이들을 일일이 설정하는것은 굉장히 번거로운 일이기 때문에 다음과 같이 방화벽을 내리는것이 편리합니다. 

`su - root`
`service iptables stop`
`chkconfig iptables off`


